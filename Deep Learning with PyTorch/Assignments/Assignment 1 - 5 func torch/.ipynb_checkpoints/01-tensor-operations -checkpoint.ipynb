{"cells":[{"cell_type":"markdown","metadata":{},"source":["# A Random Sampling of Functions for Random Sampling in PyTorch\n","\n","PyTorch is a Scientific computing package for Python that is similar to NumPy. However, PyTorch has some features that allow it to be more optimized for deep learning research with more flexibility and speed, since it allows the use of the computer's GPU for much faster processing. For my functions, I've selected items that deal with generating tensors that are filled with random numbers selected in different ways, and also breaking large tensors into more manageable pieces. I've explored some of the differences between these functions to determine their best use cases.\n","\n","- function 1 - torch.arange()\n","- function 2 - torch.rand()\n","- function 3 - torch.chunk()\n","- function 4 - torch.randint_like()\n","- function 5 - torch.randperm()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\nRequirement already satisfied: numpy in c:\\users\\geralt\\anaconda3\\lib\\site-packages (1.19.2)\nRequirement already satisfied: torch==1.7.0+cpu in c:\\users\\geralt\\anaconda3\\lib\\site-packages (1.7.0+cpu)\nRequirement already satisfied: torchvision==0.8.1+cpu in c:\\users\\geralt\\anaconda3\\lib\\site-packages (0.8.1+cpu)\nRequirement already satisfied: torchaudio==0.7.0 in c:\\users\\geralt\\anaconda3\\lib\\site-packages (0.7.0)\nRequirement already satisfied: typing-extensions in c:\\users\\geralt\\anaconda3\\lib\\site-packages (from torch==1.7.0+cpu) (3.7.4.3)\nRequirement already satisfied: future in c:\\users\\geralt\\anaconda3\\lib\\site-packages (from torch==1.7.0+cpu) (0.18.2)\nRequirement already satisfied: dataclasses in c:\\users\\geralt\\anaconda3\\lib\\site-packages (from torch==1.7.0+cpu) (0.6)\nRequirement already satisfied: pillow>=4.1.1 in c:\\users\\geralt\\anaconda3\\lib\\site-packages (from torchvision==0.8.1+cpu) (8.0.1)\n"]}],"source":["# Uncomment and run the appropriate command for your operating system, if required\n","\n","# Linux / Binder\n","# !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","# Windows\n","!pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","# MacOS\n","# !pip install numpy torch torchvision torchaudio"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Import torch and other required modules\n","import torch"]},{"cell_type":"markdown","metadata":{},"source":["## Function 1 -  .arange()\n","\n","The torch.arange function is used to create a tensor that is populated with intergers from a specified range. This is useful whenever you need to generate a tensor with a specific size and/or shape that contains numbers with the same step value. This can be used in whatever context the situation calls for. I will show this in the examples below."]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["x=  tensor([[2., 5., 1.],\n        [4., 2., 5.],\n        [9., 3., 6.]])\ny=  tensor([[2.0000, 2.5000, 3.0000],\n        [3.5000, 4.0000, 4.5000],\n        [5.0000, 5.5000, 6.0000]])\ntorch.Size([3, 3])\n"]}],"source":["x= torch.tensor([[2,5,1],\n","               [4,2,5],\n","               [9,3,6]], dtype = torch.float32)\n","s= x.shape\n","y= torch.arange(2,6.5,.5, dtype= torch.float32)\n","y=y.reshape(s)\n","print('x= ',x)\n","print('y= ',y)"]},{"cell_type":"markdown","metadata":{},"source":["Here is an example where I've created a tensor by assigning the shape, and the values. I then created a tensor with the same number of intergers as the first, and reshaped it to match the shape of x. Now these two tensors can be compared and manipulated as needed."]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 3, 5, 7, 9])\ntensor([ 2,  4,  6,  8, 10])\n"]}],"source":["x= 1\n","y=10\n","v= torch.arange(x,y,2)\n","c= torch.arange(x*2,y+1,2)\n","print(v)\n","print(c)"]},{"cell_type":"markdown","metadata":{},"source":["Here I've created two tensors using the .arange function that reference pre-assigned variables. This can be useful when creating tensors from variables that need to have different mathematical operations run before being put into the tensor."]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"arange() received an invalid combination of arguments - got (int, int, list, list), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-59-d91990c61177>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mTypeError\u001b[0m: arange() received an invalid combination of arguments - got (int, int, list, list), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"]}],"source":["\n","x=torch.arange(1,10,[1,3],[2,5])\n","y=torch.arange([1,3],[2,5])"]},{"cell_type":"markdown","metadata":{},"source":["Here I've shown that you cannot pre-define the shape of the tensor when calling .arange. This is because .arange does not take a shape or list argument in it's parameters. Instead, the tensor first needs to be created with the number of intergers, and then shaped using the .reshape method."]},{"cell_type":"markdown","metadata":{},"source":["Tensor.arange is a useful function to call whenever you need to create a tensor that contains a set of numbers in a specific range, where the intergers need to have some equal distance between them. It takes the paramaters: start, end, and step; and takes arguments for: out, dtype, layout, device, and requires_grad. It is limited by certain factors such as the fact that the number of outputs will always be the result of step size/range2-range1, and the shaping must be done after the tensor is created. "]},{"cell_type":"markdown","metadata":{},"source":["Let's save our work using Jovian before continuing."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["!pip install jovian --upgrade --quiet"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["import jovian"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"application/javascript":["window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[jovian] Attempting to save notebook..\u001b[0m\n","[jovian] Updating notebook \"aakashns/01-tensor-operations\" on https://jovian.ai/\u001b[0m\n","[jovian] Uploading notebook..\u001b[0m\n","[jovian] Capturing environment..\u001b[0m\n","[jovian] Committed successfully! https://jovian.ai/aakashns/01-tensor-operations\u001b[0m\n"]},{"data":{"text/plain":["'https://jovian.ai/aakashns/01-tensor-operations'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["jovian.commit(project='01-tensor-operations')"]},{"cell_type":"markdown","metadata":{},"source":["## Function 2 -torch.rand()\n","\n","The torch.rand function is similar to the torch.arange function. However, it has some key differences that I felt made it a good subject for my second function. I will explain how the two functions are used differently in the examples below."]},{"source":["x= torch.tensor([[2,5,1],\n","               [4,2,5],\n","               [9,3,6]], dtype = torch.float32)\n","s= x.shape\n","y= torch.rand(s)\n","print(x)\n","print(y)"],"cell_type":"code","metadata":{},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2., 5., 1.],\n        [4., 2., 5.],\n        [9., 3., 6.]])\ntensor([[0.1100, 0.6432, 0.7507],\n        [0.5848, 0.0851, 0.2927],\n        [0.6896, 0.1217, 0.5761]])\n"]}]},{"cell_type":"markdown","metadata":{},"source":["In this example I show that using the .rand function can quickly and easily generate a tensor with the same shape as x, that is populated with random intergers. It is important to note however, that while the .arange function uses intergers in a given range, the .rand function picks intergers that have a uniform distribution on the interval [0,1).  "]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.4486, 0.3708, 0.6304, 0.6606, 0.3303])\ntensor([2.0000, 2.5000, 3.0000, 3.5000, 4.0000, 4.5000, 5.0000, 5.5000, 6.0000]) \n\ntensor([0.4823, 0.7674, 0.8437, 0.6836, 0.6946, 0.5435, 0.1046, 0.8030, 0.7254,\n        0.9517])\ntensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n        0.9000])\n"]}],"source":["x=torch.rand(5, dtype=torch.float32)\n","y= torch.arange(2,6.5,.5, dtype= torch.float32)\n","x1=torch.rand(10, dtype= torch.float32)\n","y1= torch.arange(0,1,.1, dtype=torch.float32)\n","print(x)\n","print(y,'\\n')\n","print(x1)\n","print(y1)"]},{"cell_type":"markdown","metadata":{},"source":["Here I've shown that while both functions can be used to create tensors for any size, the .rand picks random values on a distribution curve from [0,1), and the .arange picks intergers that are evenly seperated between the start and end values. Thus, which one is necessary depends on the range of data you wish to generate."]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"\"check_uniform_bounds\" not implemented for 'Long'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m<ipython-input-92-f8ffe391362d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m: \"check_uniform_bounds\" not implemented for 'Long'"]}],"source":["torch.rand(1, dtype= int)"]},{"cell_type":"markdown","metadata":{},"source":["Here I show that the dtype for .rand cannot be an int. This is because the distribution it selects numbers from is between, but non-inclusive of 0 & 1. Therefore the function can yield no whole number intergers."]},{"cell_type":"markdown","metadata":{},"source":["In Summary, the .rand function is useful for creating a tensor that fits the shape of another tensor and contains values on an even distribution. This is useful for selecting weights as a starting point when beginning to train a predictive program. "]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"application/javascript":["window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[jovian] Attempting to save notebook..\u001b[0m\n","[jovian] Updating notebook \"aakashns/01-tensor-operations\" on https://jovian.ai/\u001b[0m\n","[jovian] Uploading notebook..\u001b[0m\n","[jovian] Capturing environment..\u001b[0m\n","[jovian] Committed successfully! https://jovian.ai/aakashns/01-tensor-operations\u001b[0m\n"]},{"data":{"text/plain":["'https://jovian.ai/aakashns/01-tensor-operations'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["jovian.commit(project='01-tensor-operations')"]},{"cell_type":"markdown","metadata":{},"source":["## Function 3 - torch.chunk()\n","The torch.chunk function is useful for splitting large tensors into equally sized chunks of the original. This can be used to more quickly train a predictive program, or whenever a section of data should be analyzed seperately from the larger tensor."]},{"cell_type":"code","execution_count":147,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["x=  tensor([[[ 0,  1,  2,  3,  4],\n         [ 5,  6,  7,  8,  9]],\n\n        [[10, 11, 12, 13, 14],\n         [15, 16, 17, 18, 19]]])\ny1=  tensor([[[ 0,  1,  2,  3,  4]],\n\n        [[10, 11, 12, 13, 14]]])\ny2=  tensor([[[ 5,  6,  7,  8,  9]],\n\n        [[15, 16, 17, 18, 19]]])\n"]}],"source":["x=torch.arange(0,20, dtype= int)\n","x=x.reshape(2,2,5)\n","y= torch.chunk(x,2,1)\n","y1= y[0]\n","y2= y[1]\n","\n","print('x= ',x)\n","print('y1= ',y1)\n","print('y2= ',y2)\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Here is an example of a chunk taken from the original tensor(x). The .chunk function takes the parameters: input(the original tensor), chunks(the number of pieces to split it into), and dim(the dimension along which to split the original). Here I split the original into two chunks, did so along the first dimension, and printed them out as seperate tensors for better illustration of what the parameters do."]},{"cell_type":"code","execution_count":160,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["z1=  tensor([[[ 0,  1,  2]],\n\n        [[10, 11, 12]]])\nz2=  tensor([[[ 3,  4]],\n\n        [[13, 14]]])\n"]}],"source":["z= torch.chunk( y1,2,2)\n","z1=z[0]\n","z2=z[1]\n","print('z1= ',z1)\n","print('z2= ',z2)"]},{"cell_type":"markdown","metadata":{},"source":["Here I am showing two things that I felt were important: 1. The chunks generated become new tensors when assigned to a variable, 2. It is possible to split into chunks with inequal dimensions. The former is useful if it is necessary to split a specific chunk into even smaller pieces. The second shows that the chunks may not be equal if the size along the dimension is not divisible by the number of chunks selected. In this case the LAST chunk will contain the remainder of the original tensor data after all other chunks have been equally parsed."]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"chunk(): argument 'input' (position 1) must be Tensor, not tuple","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-161-6b4612b54841>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mTypeError\u001b[0m: chunk(): argument 'input' (position 1) must be Tensor, not tuple"]}],"source":["a= torch.chunk(y,2,2)"]},{"cell_type":"markdown","metadata":{},"source":["I mentioned in the previous example that The chunks generated from a tensor become new tensors. Here I am illustrating that the results of a chunk function is actually a tuple, and aren't read tensors until the individual chunks are assigned their own variables. The result from the first example (y) is a tuple that contains a list of tensors, which then need to be accessed in some way to be read and used as tensors."]},{"cell_type":"markdown","metadata":{},"source":["In summary, the chunk function is useful to call when a tensor is too large to be effectively used in it's inital state. Chunking it into smaller tensors results in faster calculations, and is useful for breaking data into specific pieces without changing the original tensor."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"application/javascript":["window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[jovian] Attempting to save notebook..\u001b[0m\n","[jovian] Updating notebook \"aakashns/01-tensor-operations\" on https://jovian.ai/\u001b[0m\n","[jovian] Uploading notebook..\u001b[0m\n","[jovian] Capturing environment..\u001b[0m\n","[jovian] Committed successfully! https://jovian.ai/aakashns/01-tensor-operations\u001b[0m\n"]},{"data":{"text/plain":["'https://jovian.ai/aakashns/01-tensor-operations'"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["jovian.commit(project='01-tensor-operations')"]},{"cell_type":"markdown","metadata":{},"source":["## Function 4 - torch.randint_like()\n","\n","The .randint_like function allows the generation of a new tensor that has the same shape as a given tensor, but filled with random intergers generated from a given range which includes the low number but is exclusive of the high number. The randint_like function accepts parameters for: input, low, and high. "]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["x=  tensor([[[ 0,  1,  2,  3,  4],\n         [ 5,  6,  7,  8,  9]],\n\n        [[10, 11, 12, 13, 14],\n         [15, 16, 17, 18, 19]]])\ny=  tensor([[[25, 21, 23, 25, 25],\n         [20, 27, 21, 20, 23]],\n\n        [[21, 25, 29, 20, 24],\n         [25, 29, 28, 24, 25]]])\n"]}],"source":["y=torch.randint_like(x,20,30)\n","print('x= ',x)\n","print('y= ',y)"]},{"cell_type":"markdown","metadata":{},"source":["Here I've generated a tensor that has the same shape as (x), and is populated with whole number intergers selected randomly from 20-30. I'll also point out that the number 25 appears in this example much more often than the others. This seems to be the result of some interaction of the function trying to generate random numbers uniformly with the given parameters since 20 intergers were needed to fill the tensor, but a range of 10(20-29)numbers were given. I thought it was interesting that the numbers 22 & 26 were not selected at all, but 25 appears 6 times."]},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["x=  tensor([[[ 0,  1,  2,  3,  4],\n         [ 5,  6,  7,  8,  9]],\n\n        [[10, 11, 12, 13, 14],\n         [15, 16, 17, 18, 19]]])\nz=  tensor([[[10., 10., 10., 10., 10.],\n         [10., 10., 10., 10., 10.]],\n\n        [[10., 10., 10., 10., 10.],\n         [10., 10., 10., 10., 10.]]])\n"]}],"source":["z= torch.randint_like(x,10,11,dtype=torch.float32)\n","print('x= ',x)\n","print('z= ',z)"]},{"cell_type":"markdown","metadata":{},"source":["This example shows that the size of the range of numbers does not have an effect on it's ability to generate a tensor of the same shape as the input, as long as the range is at least 1. It's worth noting that using this function with a small range such as 1 or 2 will not yield a very diverse sample, however."]},{"cell_type":"code","execution_count":178,"metadata":{},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"randint_like() received an invalid combination of arguments - got (Tensor, float, float, dtype=torch.dtype), but expected one of:\n * (Tensor input, int high, *, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Tensor input, int low, int high, *, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-178-64d8734746d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mTypeError\u001b[0m: randint_like() received an invalid combination of arguments - got (Tensor, float, float, dtype=torch.dtype), but expected one of:\n * (Tensor input, int high, *, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Tensor input, int low, int high, *, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"]}],"source":["z= torch.randint_like(x, 1., 5.5,dtype=torch.float32)"]},{"cell_type":"markdown","metadata":{},"source":["This example illustrates that while the function does accept the dtype argument, the low and high parameters must be entered as whole number intergers with a range of at least 1. Entering these values as floats returns an error even when the dtype is explicitly set to float."]},{"cell_type":"markdown","metadata":{},"source":["The randint_like function has a different functionality than the other functions I've looked at for generating tensors with random values. It can both generate a tensor with a specific shape, like the .rand function, but can also accept any whole number range, like the .arange function. This makes it a useful function in cases where you need values from a range greater than [0,1), that are also randomly selected instead of stepped. "]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"application/javascript":["window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[jovian] Attempting to save notebook..\u001b[0m\n","[jovian] Updating notebook \"aakashns/01-tensor-operations\" on https://jovian.ai/\u001b[0m\n","[jovian] Uploading notebook..\u001b[0m\n","[jovian] Capturing environment..\u001b[0m\n","[jovian] Committed successfully! https://jovian.ai/aakashns/01-tensor-operations\u001b[0m\n"]},{"data":{"text/plain":["'https://jovian.ai/aakashns/01-tensor-operations'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["jovian.commit(project='01-tensor-operations')"]},{"cell_type":"markdown","metadata":{},"source":["## Function 5 - torch.randperm()\n","\n","Finally, the .randperm function. This function generates a tensor with a random permutation of intergers from [0:n-1]. It is similar in nature to the .arange function since the output will only have 1-dimension. The key difference however, is that it only takes a parameter for the upper bound of the range. "]},{"cell_type":"code","execution_count":184,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0., 2., 9., 4., 1., 6., 3., 5., 7., 8.])"]},"metadata":{},"execution_count":184}],"source":["torch.randperm(10,dtype=torch.float32)"]},{"cell_type":"markdown","metadata":{},"source":["In this example, you can see that for n=10 the tensor generated contains each number in the given range, however the order in which they appear is selected randomly, and each interger is only selected once. This means that when using this function, it's length will always be equal to it's range, and it wall always contain the numbers from [0:n-1]."]},{"cell_type":"code","execution_count":198,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["20\ntorch.Size([2, 2, 5])\ntensor([ 2, 11,  6, 19, 18, 16,  5, 17, 15,  0, 12,  9,  7,  1, 10,  4,  3, 14,\n         8, 13])\ntensor([[[ 2, 11,  6, 19, 18],\n         [16,  5, 17, 15,  0]],\n\n        [[12,  9,  7,  1, 10],\n         [ 4,  3, 14,  8, 13]]])\n"]}],"source":["n=torch.numel(x)\n","print(n)\n","rp= torch.randperm(n)\n","print(rp)\n","rp= rp.reshape(x.shape)\n","print(rp)"]},{"cell_type":"markdown","metadata":{},"source":["In this example I show how the .randperm function can be used to generate a tensor that is of the same shape as an existing tensor."]},{"cell_type":"code","execution_count":205,"metadata":{},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Trying to create tensor with negative dimension -3: [-3]","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m<ipython-input-205-a034a57a2dad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m: Trying to create tensor with negative dimension -3: [-3]"]}],"source":["torch.randperm(-3)"]},{"cell_type":"markdown","metadata":{},"source":["This example shows the limits of this function. It only accepts a paramater for the upper limit of the range (n), where n is: A positive, whole interger that is greater than zero."]},{"cell_type":"markdown","metadata":{},"source":["This function is useful when the creating a tensor that requires nonrepeating values that are randomly placed from the range [0:n-1]. "]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"application/javascript":["window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[jovian] Attempting to save notebook..\u001b[0m\n","[jovian] Updating notebook \"aakashns/01-tensor-operations\" on https://jovian.ai/\u001b[0m\n","[jovian] Uploading notebook..\u001b[0m\n","[jovian] Capturing environment..\u001b[0m\n","[jovian] Committed successfully! https://jovian.ai/aakashns/01-tensor-operations\u001b[0m\n"]},{"data":{"text/plain":["'https://jovian.ai/aakashns/01-tensor-operations'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["jovian.commit(project='01-tensor-operations')"]},{"cell_type":"markdown","metadata":{},"source":["## Conclusion\n","\n","In summary, the Pytorch computing package contains a vast array of functions, and many different functions for generating random samples of data. Each of these has unique paramaters and methods that allow for random samples to be generated in specific ways, and I feel as though I've only scratched the surface with my examples. Having now completed this assignment, and spent significant time looking at the subtle differences in these functions it is my conclusion that Pytorch is a valuable tool for DeepLearning programming. There is no universal tool for generating random samples of data. Insead we are provided with a wide variety of tools that make the task of programming much more efficient than having to hard code everything individually."]},{"cell_type":"markdown","metadata":{},"source":["## Reference Links\n","Provide links to your references and other interesting articles about tensors\n","* Official documentation for tensor operations: https://pytorch.org/docs/stable/torch.html\n","* Pytorch: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html\n","* torch.arange(): https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange\n","* torch.rand(): https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand\n","* torch.chunk(): https://pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk\n","* torch.randint_like(): https://pytorch.org/docs/stable/generated/torch.randint_like.html#torch.randint_like\n","* torch.randperm(): https://pytorch.org/docs/stable/generated/torch.randperm.html#torch.randperm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"application/javascript":["window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[jovian] Attempting to save notebook..\u001b[0m\n"]}],"source":["jovian.commit(project='01-tensor-operations')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5-final"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":4}